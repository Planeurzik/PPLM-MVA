{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Encode input text\\ninput_text = \"Today the whether is\"\\ninput_ids = tokenizer.encode(input_text, return_tensors=\\'pt\\').to(device)\\n\\n# Check if pad_token_id is None and handle it\\npad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\\nattention_mask = input_ids.ne(pad_token_id).float()\\n\\n# Generate text with adjusted parameters\\noutput = model.generate(\\n    input_ids,\\n    attention_mask=attention_mask,\\n    max_length=1000,\\n    num_return_sequences=1,\\n    do_sample=True,           # Enable sampling\\n    top_k=50,                 # Top-k sampling\\n    top_p=0.95,               # Top-p sampling\\n    temperature=0.7,          # Temperature\\n    repetition_penalty=1.2,   # Penalty for repetition\\n    pad_token_id=pad_token_id\\n)\\n\\n# Decode and print the generated text\\ngenerated_text = tokenizer.decode(output[0], skip_special_tokens=True)\\nprint(generated_text)\\n'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Encode input text\n",
    "input_text = \"Today the whether is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# Check if pad_token_id is None and handle it\n",
    "pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "attention_mask = input_ids.ne(pad_token_id).float()\n",
    "\n",
    "# Generate text with adjusted parameters\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=1000,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,           # Enable sampling\n",
    "    top_k=50,                 # Top-k sampling\n",
    "    top_p=0.95,               # Top-p sampling\n",
    "    temperature=0.7,          # Temperature\n",
    "    repetition_penalty=1.2,   # Penalty for repetition\n",
    "    pad_token_id=pad_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words: {'science': ['astronomy', 'atom', 'biology', 'cell', 'chemical', 'chemistry', 'climate', 'control', 'data', 'electricity', 'element', 'energy', 'evolution', 'experiment', 'fact', 'flask', 'fossil', 'funnel', 'genetics', 'gravity', 'hypothesis', 'lab', 'laboratory', 'laws', 'mass', 'matter', 'measure', 'microscope', 'mineral', 'molecule', 'motion', 'observe', 'organism', 'particle', 'phase', 'physics', 'research', 'scale', 'science', 'scientist', 'telescope', 'temperature', 'theory', 'tissue', 'variable', 'volume', 'weather', 'weigh'], 'legal': ['affidavit', 'allegation', 'appeal', 'appearance', 'argument', 'arrest', 'assault', 'attorney', 'bail', 'bankrupt', 'bankruptcy', 'bar', 'bench', 'warrant', 'bond', 'booking', 'capital', 'crime', 'case', 'chambers', 'claim', 'complainant', 'complaint', 'confess', 'confession', 'constitution', 'constitutional', 'contract', 'counsel', 'court', 'custody', 'damages', 'decree', 'defendant', 'defense', 'deposition', 'discovery', 'equity', 'estate', 'ethics', 'evidence', 'examination', 'family', 'law', 'felony', 'file', 'fraud', 'grievance', 'guardian', 'guilty', 'hearing', 'immunity', 'incarceration', 'incompetent', 'indictment', 'injunction', 'innocent', 'instructions', 'jail', 'judge', 'judiciary', 'jurisdiction', 'jury', 'justice', 'law', 'lawsuit', 'lawyer', 'legal', 'legislation', 'liable', 'litigation', 'manslaughter', 'mediation', 'minor', 'misdemeanor', 'moot', 'murder', 'negligence', 'oath', 'objection', 'opinion', 'order', 'ordinance', 'pardon', 'parole', 'party', 'perjury', 'petition', 'plaintiff', 'plea', 'precedent', 'prison', 'probation', 'prosecute', 'prosecutor', 'proxy', 'record', 'redress', 'resolution', 'reverse', 'revoke', 'robbery', 'rules', 'sentence', 'settlement', 'sheriff', 'sidebar', 'standing', 'state', 'statute', 'stay', 'subpoena', 'suit', 'suppress', 'sustain', 'testimony', 'theft', 'title', 'tort', 'transcript', 'trial', 'trust', 'trustee', 'venue', 'verdict', 'waiver', 'warrant', 'will', 'witness', 'writ', 'zoning'], 'military': ['academy', 'advance', 'aircraft', 'ally', 'ammo', 'ammunition', 'armor', 'arms', 'army', 'arrow', 'arsenal', 'artillery', 'attack', 'attention', 'ballistic', 'barracks', 'base', 'battalion', 'battery', 'battle', 'battlefield', 'bomb', 'bombard', 'bombardment', 'brig', 'brigade', 'bullet', 'camouflage', 'camp', 'cannon', 'captain', 'capture', 'carrier', 'casualty', 'catapult', 'cavalry', 'colonel', 'combat', 'command', 'commander', 'commission', 'company', 'conflict', 'conquest', 'convoy', 'corps', 'covert', 'crew', 'decode', 'defeat', 'defend', 'defense', 'destroyer', 'division', 'draft', 'encode', 'enemy', 'engage', 'enlist', 'evacuate', 'explosive', 'fight', 'fire', 'fleet', 'force', 'formation', 'fort', 'front', 'garrison', 'general', 'grenade', 'grunt', 'guerrilla', 'gun', 'headquarters', 'helmet', 'honor', 'hospital', 'infantry', 'injury', 'intelligence', 'invade', 'invasion', 'jet', 'kill', 'leave', 'lieutenant', 'major', 'maneuver', 'marines', 'MIA', 'mid', 'military', 'mine', 'missile', 'mortar', 'navy', 'neutral', 'offense', 'officer', 'ordinance', 'parachute', 'peace', 'plane', 'platoon', 'private', 'radar', 'rank', 'recruit', 'regiment', 'rescue', 'reserves', 'retreat', 'ribbon', 'sabotage', 'sailor', 'salute', 'section', 'sergeant', 'service', 'shell', 'shoot', 'shot', 'siege', 'sniper', 'soldier', 'spear', 'specialist', 'squad', 'squadron', 'staff', 'submarine', 'surrender', 'tactical', 'tactics', 'tank', 'torpedo', 'troops', 'truce', 'uniform', 'unit', 'veteran', 'volley', 'war', 'warfare', 'warrior', 'weapon', 'win', 'wound'], 'religion': ['Absolute', 'Affect', 'Aid', 'Angel', 'Anthem', 'Apostle', 'Archangel', 'Archbishop', 'Balance', 'Ban', 'Belief', 'Benefit', 'Bible', 'Bishop', 'Bless', 'Blessing', 'Bliss', 'Bond', 'Bow', 'Buddhism', 'Canon', 'Cantor', 'Cathedral', 'Celestial', 'Chapel', 'Charity', 'Choice', 'Christianity', 'Church', 'Comfort', 'Community', 'Conflict', 'Connection', 'Conquest', 'Conservative', 'Control', 'Conversion', 'Convert', 'Core', 'Counsel', 'Courage', 'Covenant', 'Creative', 'Creator', 'Creed', 'Cross', 'Crusade', 'Darkness', 'Decision', 'Deity', 'Destiny', 'Devil', 'Disciple', 'Discipline', 'Discussion                                                                                                                                                                                                                          [128/1838]', 'Divine', 'Divinity', 'Doctrine', 'Duty', 'Effect', 'Elder', 'Energy', 'Essence', 'Eternal', 'Ethics', 'Event', 'Evidence', 'Exile', 'Exodus', 'Faith', 'Family', 'Fate', 'Father', 'Favor', 'Fundamental', 'Gift', 'Glory', 'God', 'Gospel', 'Grace', 'Growth', 'Guru', 'Habit', 'Hallow', 'Halo', 'Happiness', 'Harmony', 'Healing', 'Heaven', 'Hebrew', 'Holy', 'Honor', 'Hope', 'Host', 'Humane', 'Immortal', 'Influence', 'Insight', 'Instruction', 'Issue', 'Jesuit', 'Jesus', 'Joy', 'Judaism', 'Judgment', 'Justice', 'Karma', 'Keen', 'Keystone', 'Kingdom                                                                                                                                                                                                                              [73/1838]', 'Latin', 'Life', 'Light', 'Love', 'Loving', 'Marriage', 'Meaning', 'Mercy', 'Messiah', 'Minister', 'Miracle', 'Mission', 'Mortal', 'Mosque', 'Movement', 'Music', 'Mystery', 'Nature', 'Nun', 'Official', 'Oracle', 'Order', 'Organ', 'Orthodox', 'Outlook', 'Pacific', 'Pagan', 'Parish', 'Participation', 'Pastor', 'Patriarch', 'Peace', 'Perception', 'Personal', 'Perspective', 'Petition', 'Pilgrim', 'Politics', 'Power', 'Practice', 'Prayer', 'Prelude', 'Presence', 'Priest', 'Principle', 'Privacy', 'Prophet', 'Protection', 'Purpose', 'Query', 'Quest', 'Question', 'Quiet', 'Radiant', 'Radical                                                                                                                                                                                                                              [18/1838]', 'Rally', 'Rebirth', 'Redemption', 'Refuge', 'Relationship', 'Relative', 'Religion', 'Religious', 'Revelation', 'Ritual', 'Role', 'Sacrament', 'Sacred', 'Sacrifice', 'Sage', 'Saint', 'Salvation', 'Sanctuary', 'Savior', 'Scripture', 'Scriptures', 'Sect', 'Security', 'Sense', 'Serious', 'Serve', 'Service', 'Sharia', 'Shepherd', 'Shrine', 'Silence', 'Sin', 'Society', 'Soul', 'Source', 'Spirit', 'Spiritual', 'Split', 'Statue', 'Sunday', 'Support', 'Supreme', 'Teaching', 'Temple', 'Tests', 'Text', 'Torah', 'Tradition', 'Traditional', 'Trust', 'Unique', 'Unity', 'Unknown', 'Value', 'Vanity', 'Virtue', 'Vision', 'Voice', 'Voices', 'Watch', 'Weight', 'Whole', 'Wisdom', 'Wonder', 'Yang', 'Yin', 'Zeal'], 'computers': ['algorithm                                                                                                                                                                                                                           [126/1852]', 'analog', 'app', 'application', 'array', 'backup', 'bandwidth', 'binary', 'bit', 'bite', 'blog', 'blogger', 'bookmark', 'boot', 'broadband', 'browser', 'buffer', 'bug', 'bus', 'byte', 'cache', 'caps', 'captcha', 'CD', 'client', 'command', 'compile', 'compress', 'computer', 'configure', 'cookie', 'copy', 'CPU', 'dashboard', 'data', 'database', 'debug', 'delete', 'desktop', 'development', 'digital', 'disk', 'document', 'domain', 'dot', 'download', 'drag', 'dynamic', 'email', 'encrypt', 'encryption', 'enter', 'FAQ', 'file', 'firewall', 'firmware                                                                                                                                                                                                                             [71/1852]', 'flaming', 'flash', 'folder', 'font', 'format', 'frame', 'graphics', 'hack', 'hacker', 'hardware', 'home', 'host', 'html', 'icon', 'inbox', 'integer', 'interface', 'Internet', 'IP', 'iteration', 'Java', 'joystick', 'kernel', 'key', 'keyboard', 'keyword', 'laptop', 'link', 'Linux', 'logic', 'login', 'lurking', 'Macintosh', 'macro', 'malware', 'media', 'memory', 'mirror', 'modem', 'monitor', 'motherboard', 'mouse', 'multimedia', 'net', 'network', 'node', 'offline', 'online', 'OS', 'option', 'output', 'page', 'password', 'paste', 'path', 'piracy', 'pirate', 'platform', 'podcast', 'portal', 'print', 'printer', 'privacy', 'process', 'program', 'programmer', 'protocol', 'RAM', 'reboot', 'resolution', 'restore', 'ROM', 'root', 'router', 'runtime', 'save', 'scan', 'scanner', 'screen', 'screenshot', 'script', 'scroll', 'security', 'server', 'shell', 'shift', 'snapshot', 'software', 'spam', 'spreadsheet', 'storage', 'surf', 'syntax', 'table', 'tag', 'template', 'thread', 'toolbar', 'trash', 'undo', 'Unix', 'upload', 'URL', 'user', 'UI', 'username', 'utility', 'version', 'virtual', 'virus', 'web', 'website', 'widget', 'wiki', 'window', 'Windows', 'wireless', 'worm', 'XML', 'Zip'], 'politics': ['affirm', 'appropriation', 'aristocracy', 'authoritarian', 'authority', 'authorization', 'brief', 'capitalism', 'communism', 'constitution', 'conservatism', 'court', 'deficit', 'diplomacy', 'direct', 'democracy', 'equality', 'exports', 'fascism', 'federation', 'government', 'ideology', 'imports', 'initiative', 'legislature', 'legitimacy', 'liberalism', 'liberty', 'majority', 'order', 'political', 'culture', 'politics', 'power', 'primary', 'property', 'ratification', 'recall', 'referendum', 'republic', 'socialism', 'state', 'subsidy', 'tariff', 'imports', 'tax', 'totalitarian'], 'space': ['planet', 'galaxy', 'space', 'universe', 'orbit', 'spacecraft', 'earth', 'moon', 'comet', 'star', 'astronaut', 'aerospace', 'asteroid', 'spaceship', 'starship', 'galactic', 'satellite', 'meteor'], 'fantasy': ['beast', 'Cerberus', 'demon', 'dragon', 'fairy', 'Frankenstein', 'ghost', 'Godzilla', 'giant', 'horror', 'hydra', 'imp', 'monster', 'mummy', 'ogre', 'orc', 'savage', 'spirit', 'sprite', 'titan', 'troll', 'undead', 'unicorn', 'vampire', 'witch', 'zombie'], 'kitchen': ['aluminum foil', 'apron', 'baking dish', 'basket', 'batter', 'beater', 'blender', 'boil', 'bottle', 'bottle opener', 'bowl', 'bread basket', 'broiler', 'broom', 'bun warmer', 'bundt pan', 'butter dish', 'cabinet', 'caddy', 'cake pan', 'cake stand', 'can', 'can opener', 'carafe', 'casserole', 'cast iron pan', 'china', 'chop', 'chopsticks', 'cleanser', 'coffee grinder', 'coffee maker', 'coffee mill', 'colander', 'cook', 'cookbook', 'cooker', 'cookie cutter', 'cookie sheet', 'corn pick', 'counter', 'creamer', 'crock-pot', 'cup', 'cupboard', 'custard cup', 'cutlery', 'cutting board', 'decanter', 'dish', 'dish rack', 'dish soap', 'dish towel', 'dishwasher', 'dough', 'Dutch oven', 'egg beater', 'egg timer', 'espresso machine', 'flatware', 'flour sifter', 'fondue set', 'food', 'food processor', 'fork', 'freezer', 'fruit bowl', 'fryer', 'frying pan', 'garbage bag', 'garbage can', 'garbage compactor', 'garbage disposal', 'garlic press', 'glasses', 'grater', 'gravy boat', 'griddle', 'grill', 'grinder', 'honey dipper', 'honey pot', 'hot plate', 'ice box', 'ice bucket', 'ice cream maker', 'ice cream scoop', 'ice cube tray', 'ice pick', 'iron skillet', 'island', 'jarjug', 'juice glass', 'juicer', 'kettle', 'kitchen', 'kitchen island', 'knife', 'knife sharpener', 'ladle', 'leftovers', 'lid', 'marinate', 'masher', 'measuring cup', 'measuring spoons', 'meat grinder', 'meat tenderizer', 'meat thermometer', 'microwave oven', 'mixer', 'mixing bowl', 'mold', 'mop', 'mortar and pestle', 'muffin pan', 'mug', 'napkin', 'nesting bowls', 'nut cracker', 'nut pick', 'opener', 'oven', 'oven mitt', 'pan', 'paper towels', 'pepper grinder', 'pepper mill', 'pepper shaker', 'percolator', 'pestle', 'pie plate', 'pitcher', 'pizza cutter', 'pizza pan', 'pizza wheel', 'placemat', 'plastic bags', 'plastic wrap', 'plate', 'platter', 'popcorn popper', 'pot', 'potato masher', 'potholder', 'poultry shears', 'pressure cooker', 'quiche pan', 'ramekin', 'range', 'reamer', 'recipe', 'refrigerator', 'rice cooker', 'roaster', 'roasting pan', 'rolling pin', 'salad bowl', 'salad spinner', 'salt shaker', 'sauce boat', 'sauce pan', 'saucer', 'serving pieces', 'serving platter', 'shears', 'shelves', 'sieve', 'sifter', 'silverware', 'sink', 'skewers', 'skillet', 'slicer', 'slow cooker', 'soap', 'soup bowl', 'spatula', 'spice jar', 'spices', 'sponge', 'spoon', 'steak knife', 'steamer', 'steel wool', 'stew pot', 'stove', 'sugar bowl', 'sugar', 'sifter', 'table', 'tablecloth', 'tablespoon', 'tea cup', 'tea infuser', 'teapot', 'tea spoon', 'thermometer', 'timer', 'tin', 'toaster', 'toaster oven', 'tongs', 'trash bags', 'trash can', 'tray', 'trivet', 'tumbler', 'tureen', 'utensils', 'vegetable bin', 'vegetable brush', 'vegetable peeler', 'waffle iron', 'waste basket', 'waxed paper', 'whip', 'whisk', 'whisk broom', 'wok', 'yogurt maker', 'zester']}\n"
     ]
    }
   ],
   "source": [
    "def create_bag_of_words(folder_path):\n",
    "    bag_of_words = {}\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        if os.path.isfile(file_path) and filename.endswith('.txt'):\n",
    "            topic = os.path.splitext(filename)[0]\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                words = file.read().splitlines()\n",
    "                bag_of_words[topic] = words\n",
    "    \n",
    "    return bag_of_words\n",
    "\n",
    "folder_path = 'wordlists'\n",
    "topic_bow = create_bag_of_words(folder_path)\n",
    "del topic_bow[\"positive_words\"]\n",
    "print(\"Bag of Words:\", topic_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_classification(text, topic_dict):\n",
    "    # Tokenize (split by space, lowercase everything)\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Count occurrences\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Score each topic by summing occurrences\n",
    "    scores = {topic: sum(word_counts[word] for word in words_set) for topic, words_set in topic_dict.items()}\n",
    "    max_topic = max(scores, key=scores.get)\n",
    "    \n",
    "    return max_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science\n",
      "military\n"
     ]
    }
   ],
   "source": [
    "text1 = \"The recent breakthrough in quantum physics has allowed scientists to explore new dimensions of energy and matter, potentially revolutionizing our understanding of the universe.\"\n",
    "text2 = \"The military has deployed advanced drone technology to enhance surveillance and reconnaissance missions, providing real-time intelligence and improving strategic decision-making.\"\n",
    "\n",
    "print(bag_of_words_classification(text1, topic_bow))\n",
    "print(bag_of_words_classification(text2, topic_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWAttributeModel:\n",
    "    def __init__(self, topics_dict, tokenizer, vocab_size=50257):\n",
    "        \"\"\"\n",
    "        Initialize the BoWAttributeModel with a dictionary of topics.\n",
    "\n",
    "        :param topics_dict: Dictionary where keys are topics and values are lists of words.\n",
    "        :param tokenizer: Tokenizer to convert words to token IDs.\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        \"\"\"\n",
    "        self.topics_dict = topics_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.topic_masks = self._build_topic_masks()\n",
    "\n",
    "    def _build_topic_masks(self):\n",
    "        \"\"\"\n",
    "        Build a mask for each topic.\n",
    "        Each mask is a tensor of shape (vocab_size,) with 1.0 for token IDs that belong to the topic and 0.0 elsewhere.\n",
    "        \"\"\"\n",
    "        topic_masks = {}\n",
    "        for topic, words in self.topics_dict.items():\n",
    "            mask = torch.zeros(self.vocab_size)\n",
    "            for word in words:\n",
    "                # Tokenize the word without adding special tokens\n",
    "                token_ids = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "                # Tokenize with space prefix (handles cases like 'Ġword')\n",
    "                token_ids_with_space = self.tokenizer.encode(\" \" + word, add_special_tokens=False)\n",
    "                # In case the word is tokenized into multiple tokens, we mark all of them.\n",
    "                for token_id in token_ids:\n",
    "                    if token_id < self.vocab_size:  # Safety check\n",
    "                        mask[token_id] = 1.0/len(words)\n",
    "            topic_masks[topic] = mask\n",
    "        return topic_masks\n",
    "\n",
    "    def forward(self, logits):\n",
    "        \"\"\"\n",
    "        Compute the log probability that the generated tokens (given by logits) belong to each topic.\n",
    "\n",
    "        :param logits: Tensor of shape (batch_size, vocab_size)\n",
    "        :return: Dictionary mapping each topic to a tensor of log probabilities of shape (batch_size,)\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities.\n",
    "        #probs = F.softmax(logits, dim=-1)  # Shape: (batch_size, nb_tokens_sentence, vocab_size)\n",
    "        log_topic_probs = {}\n",
    "        \n",
    "        for topic, mask in self.topic_masks.items():\n",
    "            mask = mask.to(logits.device)\n",
    "            topic_sum = torch.sum(logits * mask.view(1, 1, -1), dim=-1)\n",
    "            #log_topic_prob = torch.log(torch.sum(topic_sum, dim=-1))\n",
    "            log_topic_probs[topic] = topic_sum\n",
    "        \n",
    "        return log_topic_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_model = BoWAttributeModel(bow, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nphrase1 = \"The recent breakthrough in quantum physics has allowed scientists to explore new dimensions of energy and matter, potentially revolutionizing our understanding of the universe.\"\\nphrase1 = \"climate control data electricity element energy evolution experiment fact flask fossil\"\\nphrase2 = \"The military has deployed advanced drone technology to enhance surveillance and reconnaissance missions, providing real-time intelligence and improving strategic decision-making.\"\\n\\n# Encode the input text\\ninput_ids1 = tokenizer.encode(phrase1.lower(), return_tensors=\\'pt\\').to(device)\\ninput_ids2 = tokenizer.encode(phrase2.lower(), return_tensors=\\'pt\\').to(device)\\n\\n# Get logits from the model\\nwith torch.no_grad():\\n    outputs1 = model(input_ids1)\\n    outputs2 = model(input_ids2)\\n    logits1 = outputs1.logits\\n    logits2 = outputs2.logits\\n\\n# Compute log likelihoods for each topic\\nlog_probs1 = bow_model.forward(logits1)\\nlog_probs2 = bow_model.forward(logits2)\\n\\n# Print the log probabilities for each topic\\nprint(\"Log Probabilities for Phrase 1 (Science):\")\\nfor topic, log_prob in log_probs1.items():\\n    print(f\"Topic: {topic}, Log Probability: {log_prob}\")\\n\\nprint(\"\\nLog Probabilities for Phrase 2 (Military):\")\\nfor topic, log_prob in log_probs2.items():\\n    print(f\"Topic: {topic}, Log Probability: {log_prob}\")\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "phrase1 = \"The recent breakthrough in quantum physics has allowed scientists to explore new dimensions of energy and matter, potentially revolutionizing our understanding of the universe.\"\n",
    "phrase1 = \"climate control data electricity element energy evolution experiment fact flask fossil\"\n",
    "phrase2 = \"The military has deployed advanced drone technology to enhance surveillance and reconnaissance missions, providing real-time intelligence and improving strategic decision-making.\"\n",
    "\n",
    "# Encode the input text\n",
    "input_ids1 = tokenizer.encode(phrase1.lower(), return_tensors='pt').to(device)\n",
    "input_ids2 = tokenizer.encode(phrase2.lower(), return_tensors='pt').to(device)\n",
    "\n",
    "# Get logits from the model\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(input_ids1)\n",
    "    outputs2 = model(input_ids2)\n",
    "    logits1 = outputs1.logits\n",
    "    logits2 = outputs2.logits\n",
    "\n",
    "# Compute log likelihoods for each topic\n",
    "log_probs1 = bow_model.forward(logits1)\n",
    "log_probs2 = bow_model.forward(logits2)\n",
    "\n",
    "# Print the log probabilities for each topic\n",
    "print(\"Log Probabilities for Phrase 1 (Science):\")\n",
    "for topic, log_prob in log_probs1.items():\n",
    "    print(f\"Topic: {topic}, Log Probability: {log_prob}\")\n",
    "\n",
    "print(\"\\nLog Probabilities for Phrase 2 (Military):\")\n",
    "for topic, log_prob in log_probs2.items():\n",
    "    print(f\"Topic: {topic}, Log Probability: {log_prob}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
