{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pplm_classification_head import ClassificationHead  # From PPLM repo\n",
    "\n",
    "model_discrim = \"sentiment\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1949581/3272166880.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  top_k_probs = F.softmax(top_k_values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chicken is cooked to perfection that it's perfect but don't be too quick with your cooking time as it will get tough to cook to your liking.\n",
      "\n",
      "Print Healthy Salmon & Chickpea Casserole Author: Lauren\n",
      "\n",
      "Yield: Yield: 4 to 6 Ingredients Carrots: 1 small carrot, chopped coarsely\n",
      "\n",
      "Salt and freshly ground black pepper\n",
      "\n",
      "Cranberries: 1 large can of cranberries (fresh or frozen), chopped\n",
      "\n",
      "Basil: 1 large onion,\n"
     ]
    }
   ],
   "source": [
    "def load_gpt2():\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\").to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_discriminator(discrim_path):\n",
    "    discrim = ClassificationHead(class_size=5, embed_size=1024).to(device)  # Adjust class_size as needed\n",
    "    discrim.load_state_dict(torch.load(discrim_path, map_location=device))\n",
    "    discrim.eval()\n",
    "    return discrim\n",
    "\n",
    "def generate_text_with_steering(model, tokenizer, discriminator, prompt, steps=100, alpha=1):\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    output = input_ids.clone().to(device)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        outputs = model(output, return_dict=True, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1][:, -1, :].detach().requires_grad_(True)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        pred = discriminator(hidden_states)\n",
    "        target_class = torch.tensor([3], device=device)\n",
    "    \n",
    "        loss = torch.nn.CrossEntropyLoss()(pred, target_class)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Modify hidden state\n",
    "        with torch.no_grad():\n",
    "            hidden_states += alpha * hidden_states.grad\n",
    "            new_logits = model.lm_head(hidden_states)\n",
    "        \n",
    "        # Get next token\n",
    "        #next_token = torch.argmax(logits, dim=-1)[:, None]\n",
    "        \n",
    "        top_k_values, top_k_indices = torch.topk(new_logits[0,:], 100)\n",
    "        top_k_probs = F.softmax(top_k_values)\n",
    "        token_k_id = torch.multinomial(top_k_probs, num_samples=1)\n",
    "        next_token = top_k_indices[token_k_id].unsqueeze(0)\n",
    "        \n",
    "        output = torch.cat((output, next_token), dim=1)\n",
    "        \n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example Usage\n",
    "model, tokenizer = load_gpt2()\n",
    "discriminator = load_discriminator(\"discrim_models/\"+model_discrim+\"_classifierhead.pt\")\n",
    "prompt = \"The chicken is\"\n",
    "\"\"\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model(input_ids, return_dict=True, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states[-1][:, -1, :]\n",
    "logits_clone = hidden_states.clone().detach().requires_grad_(True)\n",
    "print(discriminator(logits_clone))\n",
    "\"\"\"\n",
    "generated_text = generate_text_with_steering(model, tokenizer, discriminator, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.9274,  2.5428,  3.3458, -0.4304,  5.3050]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The chicken is very tender. To serve this on toast with bread, you can put a small amount of the gravy on top, and slice the slices to make half a serving. Of course, the gravy should be cooked to the proper temperature first to get a nice crisp fresh flavor. As a quick tip, using more water to brown the chicken allows the side and meat sides to form a nice finish.The new model version 1.1 for the popular Ubuntu 14.04 Precise Pangolin distro\"\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "input_ids = input_ids.to(device)\n",
    "outputs = model(input_ids, return_dict=True, output_hidden_states=True)\n",
    "hidden_states = outputs.hidden_states[-1][:, -1, :]\n",
    "logits_clone = hidden_states.clone().detach().requires_grad_(True)\n",
    "print(discriminator(logits_clone))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
